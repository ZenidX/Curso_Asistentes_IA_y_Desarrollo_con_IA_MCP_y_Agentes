# M√≥dulo 1: APIs de IA Generativa

## Informaci√≥n del M√≥dulo

| Campo | Detalle |
|-------|---------|
| **Duraci√≥n estimada** | 3-4 horas |
| **Nivel** | Principiante |
| **Prerrequisitos** | Conocimientos b√°sicos de programaci√≥n, Python instalado |
| **Herramientas necesarias** | Python 3.8+, pip, editor de c√≥digo |

---

## Objetivos de Aprendizaje

Al completar este m√≥dulo, ser√°s capaz de:

- [ ] Entender qu√© son las APIs de IA Generativa y para qu√© sirven
- [ ] Obtener API Keys gratuitas de los principales proveedores
- [ ] Comprender los conceptos de tokens, prompts y par√°metros
- [ ] Realizar llamadas b√°sicas a las APIs de OpenAI, Anthropic, Google y Ollama
- [ ] Elegir el proveedor adecuado seg√∫n el caso de uso
- [ ] Implementar streaming y manejo de errores

---

## Continuaci√≥n del Proyecto: TaskFlow

A lo largo de los m√≥dulos, construiremos **TaskFlow**, una aplicaci√≥n de gesti√≥n de tareas. En este m√≥dulo, sentaremos las bases:

```
TaskFlow/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml       ‚Üê Configuraci√≥n de APIs
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 01_basico/        ‚Üê Primeros pasos con cada API
‚îÇ   ‚îú‚îÄ‚îÄ 02_intermedio/    ‚Üê Streaming y comparativas
‚îÇ   ‚îî‚îÄ‚îÄ 03_avanzado/      ‚Üê Function calling, embeddings
‚îî‚îÄ‚îÄ requirements.txt
```

**Objetivo del m√≥dulo**: Conectar TaskFlow con m√∫ltiples proveedores de IA para generar descripciones de tareas, sugerir prioridades y responder consultas.

---

## 1. Introducci√≥n a las APIs de IA Generativa

**Tiempo estimado: 20 minutos**

### 1.1 ¬øQu√© son las APIs de IA Generativa?

Las APIs de IA Generativa son interfaces que permiten a los desarrolladores integrar modelos de lenguaje avanzados (LLMs) en sus aplicaciones. Estos modelos pueden:

- **Generar texto**: Responder preguntas, escribir art√≠culos, c√≥digo, emails
- **Analizar contenido**: Resumir documentos, extraer informaci√≥n, clasificar texto
- **Transformar datos**: Traducir idiomas, reformatear informaci√≥n, convertir formatos
- **Razonar**: Resolver problemas l√≥gicos, matem√°ticos, de programaci√≥n

### üí° Concepto Clave

> **API (Application Programming Interface)**: Es un "contrato" entre tu c√≥digo y un servicio externo. Env√≠as datos en un formato espec√≠fico, y recibes una respuesta estructurada. No necesitas saber c√≥mo funciona el modelo internamente, solo c√≥mo comunicarte con √©l.

### 1.2 Principales Proveedores

| Proveedor | Modelos Principales | Ventajas | Ideal para |
|-----------|---------------------|----------|------------|
| **OpenAI** | GPT-4o, GPT-4o-mini | Ecosistema maduro, documentaci√≥n extensa | Producci√≥n, aplicaciones comerciales |
| **Anthropic** | Claude 3.5, Claude 3 Opus/Sonnet/Haiku | Contexto largo (200K), seguridad | An√°lisis de documentos largos |
| **Google** | Gemini 1.5 Pro/Flash | Tier gratuito generoso, multimodal | Aprendizaje, prototipos |
| **Ollama** | Llama 3.2, Mistral, CodeLlama | 100% local, privacidad, sin costes | Desarrollo offline, datos sensibles |

### üìç Checkpoint 1

Antes de continuar, responde:
- [ ] ¬øQu√© proveedor usar√≠as para analizar un libro de 500 p√°ginas?
- [ ] ¬øCu√°l es mejor para desarrollo sin conexi√≥n a internet?

<details>
<summary>Ver respuestas</summary>

- Libro de 500 p√°ginas ‚Üí **Anthropic Claude** (200K tokens) o **Google Gemini** (1M tokens)
- Desarrollo offline ‚Üí **Ollama** (modelos locales)

</details>

---

## 2. C√≥mo obtener API Keys (GRATIS)

**Tiempo estimado: 30 minutos**

### 2.1 OpenAI

**Cr√©ditos gratuitos**: $5 para nuevos usuarios

**Pasos**:
1. Reg√≠strate en [platform.openai.com](https://platform.openai.com/)
2. Ve a **API Keys** en el men√∫ lateral
3. Haz clic en **"Create new secret key"**
4. Copia la key (solo se muestra una vez)

```
Formato: sk-proj-xxxxxxxxxxxxxxxxxxxx
```

**Precios aproximados** (despu√©s del cr√©dito gratuito):
| Modelo | Input | Output |
|--------|-------|--------|
| GPT-4o-mini | $0.15 / 1M tokens | $0.60 / 1M tokens |
| GPT-4o | $2.50 / 1M tokens | $10 / 1M tokens |

### ‚ö†Ô∏è Error Com√∫n

> **No guardar la key**: OpenAI solo muestra la API key una vez. Si la pierdes, tendr√°s que generar una nueva. Gu√°rdala inmediatamente en un lugar seguro.

---

### 2.2 Anthropic (Claude)

**Cr√©ditos gratuitos**: Disponibles al registrarse

**Pasos**:
1. Reg√≠strate en [console.anthropic.com](https://console.anthropic.com/)
2. Ve a **Settings ‚Üí API Keys**
3. Haz clic en **"Create Key"**
4. Copia la key

```
Formato: sk-ant-api03-xxxxxxxxxxxxxxxxxxxx
```

**Precios aproximados**:
| Modelo | Input | Output |
|--------|-------|--------|
| Claude 3 Haiku | $0.25 / 1M tokens | $1.25 / 1M tokens |
| Claude 3.5 Sonnet | $3 / 1M tokens | $15 / 1M tokens |

---

### 2.3 Google AI Studio (Gemini)

**Tier gratuito**: MUY GENEROSO - 60 requests/minuto sin coste

**Pasos**:
1. Ve a [aistudio.google.com](https://aistudio.google.com/)
2. Haz clic en **"Get API Key"**
3. Selecciona o crea un proyecto de Google Cloud
4. Copia la key generada

```
Formato: AIzaSyXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

**L√≠mites gratuitos**:
- 60 requests por minuto
- 1 mill√≥n de tokens por minuto
- Sin l√≠mite diario

### üí° Concepto Clave

> **Por qu√© Gemini es ideal para aprender**: Su tier gratuito es el m√°s generoso del mercado. Puedes hacer miles de llamadas al d√≠a sin coste, perfecto para experimentar y aprender.

---

### 2.4 Ollama (100% GRATIS)

Ollama permite ejecutar LLMs localmente, sin necesidad de API key ni conexi√≥n a internet.

**Instalaci√≥n**:

```bash
# Windows (PowerShell como administrador)
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

**Descargar un modelo**:

```bash
# Modelo ligero (3B par√°metros, ~2GB)
ollama pull llama3.2

# Modelo para c√≥digo
ollama pull codellama

# Ver modelos disponibles
ollama list
```

**Verificar instalaci√≥n**:

```bash
ollama run llama3.2 "Hola, ¬øc√≥mo est√°s?"
```

### üéØ Pr√°ctica Guiada 1: Configurar tu Entorno

1. Elige al menos 2 proveedores
2. Obt√©n las API keys siguiendo los pasos anteriores
3. Crea el archivo de configuraci√≥n:

```bash
cd TaskFlow
cp config/config.example.yaml config/config.yaml
```

4. A√±ade tus keys al archivo `config/config.yaml`
5. Verifica que no est√© en git: `cat .gitignore | grep config.yaml`

**Criterios de √©xito**:
- [ ] Tienes al menos 2 API keys
- [ ] El archivo config.yaml tiene tus keys
- [ ] El archivo NO est√° en control de versiones

---

## 3. Conceptos Fundamentales

**Tiempo estimado: 45 minutos**

### 3.1 Tokens

Los tokens son las unidades b√°sicas que procesan los LLMs. No son exactamente palabras ni caracteres.

**Regla aproximada**:
- Ingl√©s: 1 token ‚âà 4 caracteres ‚âà 0.75 palabras
- Espa√±ol: 1 token ‚âà 3-4 caracteres (var√≠a m√°s por acentos)

**Ejemplos de tokenizaci√≥n**:

```
"Hola mundo"              ‚Üí ["Hola", " mundo"]           ‚Üí 2 tokens
"Inteligencia artificial" ‚Üí ["Int", "elig", "encia", " artificial"] ‚Üí 4 tokens
"12345"                   ‚Üí ["123", "45"]                ‚Üí 2 tokens
```

### üí° Concepto Clave

> **¬øPor qu√© importan los tokens?**
> 1. **Coste**: Se paga por token procesado
> 2. **L√≠mites**: Los modelos tienen contexto m√°ximo en tokens
> 3. **Velocidad**: M√°s tokens = respuesta m√°s lenta

**Regla pr√°ctica**: 100 tokens ‚âà 75 palabras ‚âà 1/4 de p√°gina A4

---

### 3.2 Prompts y Completions

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PROMPT (Input)                         ‚îÇ
‚îÇ  "Explica qu√© es Python en 2 frases"    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ            ‚Üì Modelo LLM ‚Üì               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  COMPLETION (Output)                    ‚îÇ
‚îÇ  "Python es un lenguaje de programaci√≥n ‚îÇ
‚îÇ   interpretado y de alto nivel..."      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Tipos de prompts**:

| Tipo | Descripci√≥n | Ejemplo |
|------|-------------|---------|
| **System** | Define el comportamiento del modelo | "Eres un asistente experto en Python" |
| **User** | El mensaje del usuario | "¬øC√≥mo ordeno una lista?" |
| **Assistant** | Respuestas previas (para contexto) | "Puedes usar sort() o sorted()..." |

---

### 3.3 Par√°metros Principales

#### Temperature (0.0 - 2.0)

Controla la aleatoriedad/creatividad de las respuestas.

| Valor | Comportamiento | Uso recomendado |
|-------|----------------|-----------------|
| 0.0 | Determinista, siempre igual | C√≥digo, datos estructurados |
| 0.3-0.5 | Consistente con ligera variaci√≥n | Res√∫menes, traducciones |
| 0.7 | Equilibrado (default) | Conversaci√≥n general |
| 1.0-1.5 | Creativo | Escritura creativa, brainstorming |
| 2.0 | Muy aleatorio | Experimentaci√≥n |

```python
# Ejemplo: Mismo prompt, diferente temperature
prompt = "Escribe un t√≠tulo para un art√≠culo sobre IA"

# temperature=0: "Inteligencia Artificial: Una Gu√≠a Completa"
# temperature=0.7: "El Futuro es Ahora: C√≥mo la IA Est√° Cambiando Todo"
# temperature=1.5: "¬°Robots Pensantes! La Revoluci√≥n Silenciosa"
```

#### Max Tokens

L√≠mite m√°ximo de tokens en la respuesta.

```python
max_tokens=50   # Respuestas cortas (1-2 oraciones)
max_tokens=500  # Respuestas medianas (1-2 p√°rrafos)
max_tokens=4096 # Respuestas largas (art√≠culos)
```

### ‚ö†Ô∏è Error Com√∫n

> **Respuesta cortada abruptamente**: Si el modelo alcanza `max_tokens`, la respuesta se corta sin aviso. Aumenta el l√≠mite si ves respuestas incompletas.

#### Top P (Nucleus Sampling)

Alternativa a temperature. Selecciona tokens cuya probabilidad acumulada suma P.

```python
top_p=0.1  # Solo considera el 10% m√°s probable ‚Üí Muy conservador
top_p=0.9  # Considera el 90% m√°s probable ‚Üí M√°s variado
top_p=1.0  # Considera todos los tokens (default)
```

**Recomendaci√≥n**: Usa `temperature` O `top_p`, no ambos a la vez.

---

### 3.4 Ventana de Contexto

Cada modelo tiene un l√≠mite de tokens totales (input + output).

| Modelo | Contexto M√°ximo | Equivalente |
|--------|-----------------|-------------|
| GPT-4o | 128K tokens | ~400 p√°ginas |
| GPT-4o-mini | 128K tokens | ~400 p√°ginas |
| Claude 3.5 Sonnet | 200K tokens | ~600 p√°ginas |
| Gemini 1.5 Pro | 1M tokens | ~3,000 p√°ginas |
| Llama 3.2 (Ollama) | 128K tokens | ~400 p√°ginas |

### üìç Checkpoint 2

Responde:
- [ ] ¬øQu√© temperature usar√≠as para generar c√≥digo?
- [ ] Si una respuesta se corta a la mitad, ¬øqu√© par√°metro debes ajustar?
- [ ] ¬øCu√°ntas p√°ginas aproximadamente caben en 100K tokens?

<details>
<summary>Ver respuestas</summary>

- C√≥digo ‚Üí **temperature=0** (determinista, consistente)
- Respuesta cortada ‚Üí Aumentar **max_tokens**
- 100K tokens ‚Üí Aproximadamente **300 p√°ginas**

</details>

---

## 4. Estructura de las APIs

**Tiempo estimado: 40 minutos**

### 4.1 Autenticaci√≥n

Todas las APIs (excepto Ollama) requieren autenticaci√≥n via HTTP headers.

```python
# OpenAI
headers = {"Authorization": "Bearer sk-..."}

# Anthropic
headers = {"x-api-key": "sk-ant-...", "anthropic-version": "2023-06-01"}

# Google
# Se pasa como par√°metro: ?key=AIza...
```

---

### 4.2 Endpoint Principal: Chat Completions

#### OpenAI

`POST https://api.openai.com/v1/chat/completions`

```json
{
  "model": "gpt-4o-mini",
  "messages": [
    {"role": "system", "content": "Eres un asistente √∫til."},
    {"role": "user", "content": "¬øQu√© es Python?"}
  ],
  "temperature": 0.7,
  "max_tokens": 500
}
```

#### Anthropic

`POST https://api.anthropic.com/v1/messages`

```json
{
  "model": "claude-3-haiku-20240307",
  "max_tokens": 500,
  "system": "Eres un asistente √∫til.",
  "messages": [
    {"role": "user", "content": "¬øQu√© es Python?"}
  ]
}
```

#### Google

`POST https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent`

```json
{
  "contents": [
    {"role": "user", "parts": [{"text": "¬øQu√© es Python?"}]}
  ],
  "generationConfig": {
    "temperature": 0.7,
    "maxOutputTokens": 500
  }
}
```

#### Ollama

`POST http://localhost:11434/api/chat`

```json
{
  "model": "llama3.2",
  "messages": [
    {"role": "system", "content": "Eres un asistente √∫til."},
    {"role": "user", "content": "¬øQu√© es Python?"}
  ],
  "stream": false
}
```

---

### 4.3 Formato de Respuesta

**Estructura com√∫n** (simplificada):

```json
{
  "id": "chatcmpl-xxx",
  "model": "gpt-4o-mini",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "Python es un lenguaje de programaci√≥n..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 150,
    "total_tokens": 175
  }
}
```

**Campos importantes**:

| Campo | Descripci√≥n | Uso |
|-------|-------------|-----|
| `content` | La respuesta generada | Mostrar al usuario |
| `finish_reason` | Por qu√© termin√≥ | `stop` (normal), `length` (cortado), `content_filter` (bloqueado) |
| `usage` | Tokens consumidos | Calcular costes, monitorear uso |

---

### üéØ Pr√°ctica Guiada 2: Tu Primera Llamada a la API

```python
# scripts/01_basico/primera_llamada.py
import os
from openai import OpenAI

# Cargar API key desde variable de entorno
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Hacer la llamada
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "Eres un asistente experto en Python."},
        {"role": "user", "content": "¬øC√≥mo ordeno una lista en Python?"}
    ],
    temperature=0.7,
    max_tokens=500
)

# Extraer y mostrar la respuesta
print(response.choices[0].message.content)
print(f"\nTokens usados: {response.usage.total_tokens}")
```

**Pasos**:
1. Crea el archivo `scripts/01_basico/primera_llamada.py`
2. Configura la variable de entorno: `export OPENAI_API_KEY="tu-key"`
3. Ejecuta: `python scripts/01_basico/primera_llamada.py`

**Criterios de √©xito**:
- [ ] El script se ejecuta sin errores
- [ ] Recibes una respuesta del modelo
- [ ] Puedes ver los tokens consumidos

---

## 5. Comparativa de Proveedores

**Tiempo estimado: 20 minutos**

### 5.1 Cu√°ndo usar cada uno

| Necesidad | Recomendaci√≥n | Raz√≥n |
|-----------|---------------|-------|
| Aprender/Experimentar gratis | **Google Gemini** | Tier gratuito m√°s generoso |
| Privacidad total / Offline | **Ollama** | 100% local |
| M√°xima calidad | **Claude 3.5 Sonnet** o **GPT-4o** | Mejores resultados |
| Coste m√≠nimo + buena calidad | **GPT-4o-mini** o **Claude 3 Haiku** | Balance precio/rendimiento |
| Contexto muy largo | **Gemini 1.5 Pro** | 1M tokens |
| Generaci√≥n de c√≥digo | **GPT-4o** o **Claude 3.5 Sonnet** | Optimizados para c√≥digo |

### 5.2 L√≠mites de Rate

| Proveedor | Tier Gratuito | Requests/minuto |
|-----------|---------------|-----------------|
| OpenAI | Limitado tras $5 | 3-500 (seg√∫n tier) |
| Anthropic | Cr√©ditos iniciales | 5-50 (seg√∫n tier) |
| Google | Muy generoso | 60 |
| Ollama | Ilimitado | Solo limitado por hardware |

---

### üìç Checkpoint 3

Responde:
- [ ] ¬øQu√© proveedor elegir√≠as para un proyecto sin presupuesto?
- [ ] ¬øCu√°l usar√≠as para procesar documentos confidenciales de una empresa?
- [ ] ¬øQu√© modelo tiene el contexto m√°s grande?

<details>
<summary>Ver respuestas</summary>

- Sin presupuesto ‚Üí **Google Gemini** (tier gratuito generoso) u **Ollama** (gratis)
- Documentos confidenciales ‚Üí **Ollama** (datos nunca salen de tu m√°quina)
- Mayor contexto ‚Üí **Gemini 1.5 Pro** (1M tokens)

</details>

---

## 6. Ejercicios Pr√°cticos

**Tiempo estimado: 60 minutos**

### Estructura de los Scripts

```
scripts/
‚îú‚îÄ‚îÄ 01_basico/           # Primeros pasos
‚îÇ   ‚îú‚îÄ‚îÄ openai_basico.py
‚îÇ   ‚îú‚îÄ‚îÄ anthropic_basico.py
‚îÇ   ‚îú‚îÄ‚îÄ google_basico.py
‚îÇ   ‚îî‚îÄ‚îÄ ollama_basico.py
‚îú‚îÄ‚îÄ 02_intermedio/       # T√©cnicas intermedias
‚îÇ   ‚îú‚îÄ‚îÄ comparar_modelos.py
‚îÇ   ‚îú‚îÄ‚îÄ parametros_avanzados.py
‚îÇ   ‚îî‚îÄ‚îÄ streaming.py
‚îî‚îÄ‚îÄ 03_avanzado/         # Funcionalidades avanzadas
    ‚îú‚îÄ‚îÄ function_calling.py
    ‚îî‚îÄ‚îÄ embeddings.py
```

### Configuraci√≥n Inicial

```bash
# 1. Copia el archivo de configuraci√≥n
cp config/config.example.yaml config/config.yaml

# 2. A√±ade tus API keys en config/config.yaml

# 3. Instala dependencias
pip install -r requirements.txt

# 4. Ejecuta tu primer script
python scripts/01_basico/openai_basico.py
```

### Progresi√≥n Recomendada

| Nivel | Script | Qu√© aprender√°s |
|-------|--------|----------------|
| **B√°sico** | `openai_basico.py` | Llamada simple, extraer respuesta |
| **B√°sico** | `anthropic_basico.py` | Diferencias con OpenAI |
| **B√°sico** | `google_basico.py` | Estructura de Gemini |
| **B√°sico** | `ollama_basico.py` | Modelos locales |
| **Intermedio** | `comparar_modelos.py` | Mismo prompt, diferentes modelos |
| **Intermedio** | `streaming.py` | Respuestas en tiempo real |
| **Avanzado** | `function_calling.py` | LLMs que usan herramientas |
| **Avanzado** | `embeddings.py` | B√∫squeda sem√°ntica |

---

### üéØ Pr√°ctica Guiada 3: Comparar Modelos

Crea un script que env√≠e el mismo prompt a diferentes proveedores y compare:

```python
# scripts/02_intermedio/comparar_modelos.py
import time
from openai import OpenAI
from anthropic import Anthropic

prompt = "Explica qu√© es recursi√≥n en programaci√≥n en 3 oraciones."

# OpenAI
start = time.time()
openai_response = # ... tu c√≥digo
openai_time = time.time() - start

# Anthropic
start = time.time()
anthropic_response = # ... tu c√≥digo
anthropic_time = time.time() - start

# Comparar
print("=== OpenAI ===")
print(f"Tiempo: {openai_time:.2f}s")
print(f"Respuesta: {openai_response}")

print("\n=== Anthropic ===")
print(f"Tiempo: {anthropic_time:.2f}s")
print(f"Respuesta: {anthropic_response}")
```

**Criterios de √©xito**:
- [ ] El script llama a ambos proveedores
- [ ] Mide el tiempo de cada uno
- [ ] Muestra ambas respuestas para comparar

---

## 7. Troubleshooting

### Errores Comunes

| Error | Causa | Soluci√≥n |
|-------|-------|----------|
| `401 Unauthorized` | API key inv√°lida o expirada | Regenerar la key |
| `429 Rate Limit` | Demasiadas peticiones | Esperar o usar otro tier |
| `400 Bad Request` | Formato incorrecto | Revisar estructura del JSON |
| `context_length_exceeded` | Prompt muy largo | Reducir tokens de entrada |

### Comandos de Diagn√≥stico

```bash
# Verificar que Ollama est√° corriendo
curl http://localhost:11434/api/version

# Probar conexi√≥n a OpenAI
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"

# Ver modelos disponibles en Ollama
ollama list
```

---

## Resumen del M√≥dulo

### Lo que aprendiste

1. **APIs de IA Generativa**: Qu√© son y para qu√© sirven
2. **API Keys**: C√≥mo obtenerlas gratis de cada proveedor
3. **Tokens**: Unidades de procesamiento, c√°lculo de costes
4. **Par√°metros**: temperature, max_tokens, top_p
5. **Estructura de APIs**: Requests y responses de cada proveedor
6. **Comparativa**: Cu√°ndo usar cada proveedor

### Preparaci√≥n para el M√≥dulo 2

En el pr√≥ximo m√≥dulo aprender√°s a usar **CLIs de IA** (Claude Code, Gemini CLI, OpenCode) que te permiten:
- Interactuar con IA desde tu terminal
- Analizar y modificar c√≥digo autom√°ticamente
- Ejecutar comandos basados en lenguaje natural

**Tarea previa**: Ten al menos una API key funcionando y haber ejecutado un script b√°sico.

---

## Recursos Adicionales

- [Documentaci√≥n OpenAI](https://platform.openai.com/docs)
- [Documentaci√≥n Anthropic](https://docs.anthropic.com/)
- [Documentaci√≥n Google AI](https://ai.google.dev/docs)
- [Documentaci√≥n Ollama](https://github.com/ollama/ollama)
- [Tokenizer Online](https://platform.openai.com/tokenizer) - Visualiza tokenizaci√≥n
