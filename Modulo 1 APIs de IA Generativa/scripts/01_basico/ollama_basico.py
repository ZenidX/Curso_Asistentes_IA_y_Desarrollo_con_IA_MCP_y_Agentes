"""
Script b√°sico para usar Ollama (Modelos Locales)
================================================

Este script demuestra c√≥mo:
1. Cargar la configuraci√≥n desde un archivo YAML
2. Conectarse a Ollama (local)
3. Enviar un prompt y recibir una respuesta
4. Manejar errores b√°sicos

REQUISITOS:
- Tener Ollama instalado (https://ollama.ai/)
- Haber descargado un modelo: ollama pull llama3.2
"""

import yaml
from pathlib import Path
import ollama


def cargar_config() -> dict:
    """Carga la configuraci√≥n desde config.yaml"""
    config_path = Path(__file__).parent.parent.parent / "config" / "config.yaml"

    if not config_path.exists():
        raise FileNotFoundError(
            f"No se encontr√≥ {config_path}\n"
            "Copia config.example.yaml a config.yaml y a√±ade tus API keys."
        )

    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def chat_con_ollama(prompt: str, config: dict) -> str:
    """
    Env√≠a un prompt a Ollama (local) y devuelve la respuesta.

    Args:
        prompt: El mensaje del usuario
        config: Configuraci√≥n con modelo y par√°metros

    Returns:
        La respuesta del modelo
    """
    # Obtener configuraci√≥n de Ollama
    ollama_config = config["apis"]["ollama"]
    modelo = ollama_config.get("default_model", "llama3.2")

    # Obtener par√°metros por defecto
    defaults = config.get("defaults", {})

    # Realizar la petici√≥n
    response = ollama.chat(
        model=modelo,
        messages=[
            {"role": "system", "content": "Eres un asistente √∫til que responde en espa√±ol."},
            {"role": "user", "content": prompt}
        ],
        options={
            "temperature": defaults.get("temperature", 0.7),
            "num_predict": defaults.get("max_tokens", 1024)
        }
    )

    # Extraer y devolver el contenido de la respuesta
    return response["message"]["content"]


def verificar_ollama_disponible() -> bool:
    """Verifica si Ollama est√° corriendo y disponible."""
    try:
        ollama.list()
        return True
    except Exception:
        return False


def listar_modelos_disponibles() -> list:
    """Lista los modelos instalados en Ollama."""
    try:
        response = ollama.list()
        return [model["name"] for model in response["models"]]
    except Exception:
        return []


def main():
    print("=" * 60)
    print("Ollama (Local) - Script B√°sico")
    print("=" * 60)

    # Verificar que Ollama est√° corriendo
    if not verificar_ollama_disponible():
        print("‚úó Error: Ollama no est√° disponible.")
        print("  Aseg√∫rate de que Ollama est√° instalado y corriendo.")
        print("  Instalaci√≥n: https://ollama.ai/")
        return

    print("‚úì Ollama est√° disponible")

    # Listar modelos disponibles
    modelos = listar_modelos_disponibles()
    if modelos:
        print(f"‚úì Modelos instalados: {', '.join(modelos)}")
    else:
        print("‚úó No hay modelos instalados. Ejecuta: ollama pull llama3.2")
        return

    # Cargar configuraci√≥n
    try:
        config = cargar_config()
        print("‚úì Configuraci√≥n cargada correctamente")
    except FileNotFoundError as e:
        print(f"‚úó Error: {e}")
        return

    # Verificar que el modelo configurado est√° disponible
    modelo_config = config["apis"]["ollama"].get("default_model", "llama3.2")
    if not any(modelo_config in m for m in modelos):
        print(f"‚ö† Modelo '{modelo_config}' no encontrado. Usando el primero disponible.")
        config["apis"]["ollama"]["default_model"] = modelos[0].split(":")[0]

    # Prompt de ejemplo
    prompt = "¬øQu√© es la inteligencia artificial? Expl√≠calo en 3 frases."
    print(f"\nüìù Prompt: {prompt}")
    print("-" * 60)

    # Obtener respuesta
    try:
        modelo = config["apis"]["ollama"].get("default_model", "llama3.2")
        print(f"ü§ñ Modelo: {modelo}")
        print("‚è≥ Esperando respuesta (local, puede tardar unos segundos)...\n")

        respuesta = chat_con_ollama(prompt, config)

        print("üí¨ Respuesta:")
        print(respuesta)

    except Exception as e:
        print(f"‚úó Error al llamar a Ollama: {e}")

    print("\n" + "=" * 60)


if __name__ == "__main__":
    main()
